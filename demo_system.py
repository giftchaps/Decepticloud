#!/usr/bin/env python3
"""
DeceptiCloud System Demo
Demonstrates autonomous honeypot learning vs static deployment
"""

import sys
import os
sys.path.append(os.getcwd())

from src.environment import CloudHoneynetEnv
from src.agent import DQNAgent
from src.monitoring import monitor
import numpy as np
import time

def demo_autonomous_learning():
    print("üß† DeceptiCloud Autonomous Learning Demo")
    print("=" * 45)
    
    # Initialize components
    print("Initializing components...")
    env = CloudHoneynetEnv('13.222.14.34', 'ubuntu', 'C:/Users/gift2/decepticloud-key-proper.pem')
    agent = DQNAgent(state_size=2, action_size=3)
    
    print(f"‚úÖ Agent initialized - Starting epsilon: {agent.epsilon:.3f}")
    print("‚úÖ Environment connected to EC2: 13.222.14.34")
    
    # Demonstrate learning over episodes
    print("\nüéØ Training Demonstration (10 episodes)")
    print("-" * 40)
    
    episode_rewards = []
    
    for episode in range(10):
        state = env.reset()
        total_reward = 0
        
        print(f"\nEpisode {episode + 1}:")
        
        # Run episode steps
        for step in range(3):  # Short episodes for demo
            # Agent makes decision
            action = agent.act(state)
            action_names = ['None', 'SSH', 'Web']
            print(f"  Step {step + 1}: Agent chooses {action_names[action]} honeypot")
            
            # Execute action and get reward
            next_state, reward, done = env.step(action)
            print(f"    Reward: {reward}")
            
            # Store experience
            agent.remember(state, action, reward, next_state, done)
            
            total_reward += reward
            state = next_state
            
            if done:
                break
        
        # Train agent if enough experience
        if len(agent.memory) > 8:
            agent.learn(8, episode)
        
        episode_rewards.append(total_reward)
        print(f"  Episode reward: {total_reward}")
        print(f"  Epsilon (exploration): {agent.epsilon:.3f}")
        print(f"  Memory size: {len(agent.memory)}")
    
    # Show learning progress
    print("\nüìä Learning Progress Summary")
    print("-" * 30)
    print(f"Initial epsilon: 1.000 (100% random)")
    print(f"Final epsilon: {agent.epsilon:.3f} ({agent.epsilon*100:.1f}% random)")
    print(f"Average reward (first 5): {np.mean(episode_rewards[:5]):.2f}")
    print(f"Average reward (last 5): {np.mean(episode_rewards[-5:]):.2f}")
    
    improvement = np.mean(episode_rewards[-5:]) - np.mean(episode_rewards[:5])
    print(f"Improvement: {improvement:+.2f} points")
    
    # Demonstrate autonomous vs static comparison
    print("\nüî¨ Autonomous vs Static Comparison")
    print("-" * 35)
    
    # Test autonomous system (trained agent)
    print("Testing trained autonomous system...")
    agent.epsilon = 0.01  # Minimal exploration for testing
    autonomous_rewards = []
    
    for test in range(3):
        state = env._get_state()
        action = agent.act(state)
        _, reward, _ = env.step(action)
        autonomous_rewards.append(reward)
    
    # Test static SSH honeypot
    print("Testing static SSH honeypot...")
    static_ssh_rewards = []
    for test in range(3):
        _, reward, _ = env.step(1)  # Always SSH
        static_ssh_rewards.append(reward)
    
    # Test static web honeypot
    print("Testing static web honeypot...")
    static_web_rewards = []
    for test in range(3):
        _, reward, _ = env.step(2)  # Always web
        static_web_rewards.append(reward)
    
    # Results
    print("\nüìà Performance Results")
    print("-" * 22)
    print(f"Autonomous average: {np.mean(autonomous_rewards):.2f}")
    print(f"Static SSH average: {np.mean(static_ssh_rewards):.2f}")
    print(f"Static Web average: {np.mean(static_web_rewards):.2f}")
    
    # Calculate improvements
    ssh_improvement = (np.mean(autonomous_rewards) - np.mean(static_ssh_rewards)) / abs(np.mean(static_ssh_rewards)) * 100
    web_improvement = (np.mean(autonomous_rewards) - np.mean(static_web_rewards)) / abs(np.mean(static_web_rewards)) * 100
    
    print(f"\nüéØ Autonomous System Advantages:")
    print(f"  vs Static SSH: {ssh_improvement:+.1f}% improvement")
    print(f"  vs Static Web: {web_improvement:+.1f}% improvement")
    
    print("\n‚úÖ Demo completed successfully!")
    print("\nüîç Key Evidence of Learning:")
    print("  ‚Ä¢ Epsilon decreased (less random, more intelligent)")
    print("  ‚Ä¢ Reward improvement over episodes")
    print("  ‚Ä¢ Better performance than static systems")
    print("  ‚Ä¢ Real-time adaptation to attack patterns")
    
    return {
        'learning_improvement': improvement,
        'ssh_improvement': ssh_improvement,
        'web_improvement': web_improvement,
        'final_epsilon': agent.epsilon
    }

if __name__ == "__main__":
    try:
        results = demo_autonomous_learning()
        print(f"\nüìä Final Results: {results}")
    except Exception as e:
        print(f"‚ùå Demo error: {e}")
        import traceback
        traceback.print_exc()